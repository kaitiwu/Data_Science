{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words (BoW)\n",
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"bag_of_words.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 BoW: What & Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modeling text data**\n",
    ">- text data are usually messy, while ML algorithms prefer well-defined fixed-length inputs\n",
    ">- ML algorithms cannot work with raw text directly\n",
    ">- text must be converted into numbers, specifically vectors of numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature extraction**\n",
    ">- various linguistic properties of text reflected in vectors derived from text data\n",
    ">- BoW is a popular and simple method of feature extraction with text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BoW approach**\n",
    ">- a BoW is a representation of text that describes the occurrence of words within a document\n",
    ">- involves two things:\n",
    "    + a vocabulary of known words\n",
    "    + a measure of the presence of known words\n",
    ">- information about the order or structure of words in the document is discarded \n",
    ">- intuition is that documents are similar if they have similar content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros of BoW**\n",
    ">- works for any text, easy to understand and implement\n",
    ">- does not require a language model (no training)\n",
    ">- successful in problems such as language modeling and document classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cons of BoW**\n",
    ">- all words are equally (dis)similar (discrete, orthogonal vectors)\n",
    ">- ignores the context by discarding order of words\n",
    ">- computationally challenging because of sparse data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NLP Jargon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">|   terminology   |                meaning                  |\n",
    " |:---------------:|:---------------------------------------:|\n",
    " |    `corpus`     |   a list of strings or text documents   |\n",
    " | `tokenization`  |dividing text into words (or other units)|\n",
    " |    `n-grams`    |  tokenizing into strings with n words   |\n",
    " |   `stop words`  |frequent words that carry little meaning |\n",
    " |    `stemming`   |         cutting off word endings        |\n",
    " | `lemmatization` | grouping together of different forms of the same word |\n",
    " | `vectorization` |       converting text into numbers      |\n",
    " |     `tf-idf`    |    method for normalizing token counts  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Text Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beatles corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEATLES_CORPUS = [\n",
    "    \"Yesterday, all my troubles seemed so far away\", \n",
    "    \"We all live in a yellow submarine, yellow submarine\",\n",
    "    \"When I find myself in times of trouble, mother mary comes to me\",\n",
    "    \"Penny lane is in my ears and in my eyes\",\n",
    "    \"Here comes the sun and I say it's alright little darling\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Backstreet Boys corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKSTREET_BOYS_CORPUS = [       \n",
    "    \"You're the one for me you're my ecstasy youre the one I need hey yeah ohh\",\n",
    "    \"You're my fire the one desire believe me when I say I want it that way\",\n",
    "    \"Everybody rock your body, everybody rock your body, right backstreets back alright\",\n",
    "    \"Show me the meaning of being lonely is this the feeling i need to walk with\",\n",
    "    \"Now I can see that weve fallen apart from the way that it used to be yeah\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total corpus & label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "CORPUS = BEATLES_CORPUS + BACKSTREET_BOYS_CORPUS\n",
    "\n",
    "# label\n",
    "l1,l2 = len(BEATLES_CORPUS), len(BACKSTREET_BOYS_CORPUS)\n",
    "LABELS = [f\"beatles_{i}\" for i in range(l1)] + [f\"bboys_{i}\" for i in range(l2)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Yesterday, all my troubles seemed so far away',\n",
       " 'We all live in a yellow submarine, yellow submarine',\n",
       " 'When I find myself in times of trouble, mother mary comes to me',\n",
       " 'Penny lane is in my ears and in my eyes',\n",
       " \"Here comes the sun and I say it's alright little darling\",\n",
       " \"You're the one for me you're my ecstasy youre the one I need hey yeah ohh\",\n",
       " \"You're my fire the one desire believe me when I say I want it that way\",\n",
       " 'Everybody rock your body, everybody rock your body, right backstreets back alright',\n",
       " 'Show me the meaning of being lonely is this the feeling i need to walk with',\n",
       " 'Now I can see that weve fallen apart from the way that it used to be yeah']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beatles_0',\n",
       " 'beatles_1',\n",
       " 'beatles_2',\n",
       " 'beatles_3',\n",
       " 'beatles_4',\n",
       " 'bboys_0',\n",
       " 'bboys_1',\n",
       " 'bboys_2',\n",
       " 'bboys_3',\n",
       " 'bboys_4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BoW from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data analysis stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# text-related stack\n",
    "import re\n",
    "from sklearn.feature_extraction import text\n",
    "\n",
    "# miscellaneous\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Convert to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowercase_corpus = [line.lower() for line in CORPUS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yesterday, all my troubles seemed so far away',\n",
       " 'we all live in a yellow submarine, yellow submarine',\n",
       " 'when i find myself in times of trouble, mother mary comes to me',\n",
       " 'penny lane is in my ears and in my eyes',\n",
       " \"here comes the sun and i say it's alright little darling\",\n",
       " \"you're the one for me you're my ecstasy youre the one i need hey yeah ohh\",\n",
       " \"you're my fire the one desire believe me when i say i want it that way\",\n",
       " 'everybody rock your body, everybody rock your body, right backstreets back alright',\n",
       " 'show me the meaning of being lonely is this the feeling i need to walk with',\n",
       " 'now i can see that weve fallen apart from the way that it used to be yeah']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowercase_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    This function spits out a list of tokens of a string\n",
    "    \"\"\"\n",
    "    # pre-compile pattern, so that code runs faster\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    \n",
    "    # make an iterator of all match objects\n",
    "    matches = token_pattern.finditer(text)\n",
    "    \n",
    "    # create a list of all tokens\n",
    "    token_list = [match.group() for match in matches]\n",
    "    \n",
    "    return token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yesterday', 'all', 'my', 'troubles', 'seemed', 'so', 'far', 'away']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_corpus = [tokenize(line) for line in lowercase_corpus]\n",
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = text.ENGLISH_STOP_WORDS\n",
    "# must cast the text vec into lists, otherwise will cause \n",
    "## \"TypeError\" when working with count_vectorizer. \n",
    "## solution: https://stackoverflow.com/questions/75643277/how-can-i-solve-the-error-the-stop-words-parameter-of-tfidfvectorizer-must-be\n",
    "english_stop_words = list(english_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['yesterday', 'troubles', 'far', 'away'],\n",
       " ['live', 'yellow', 'submarine', 'yellow', 'submarine'],\n",
       " ['times', 'trouble', 'mother', 'mary', 'comes'],\n",
       " ['penny', 'lane', 'ears', 'eyes'],\n",
       " ['comes', 'sun', 'say', 'alright', 'little', 'darling'],\n",
       " ['ecstasy', 'youre', 'need', 'hey', 'yeah', 'ohh'],\n",
       " ['desire', 'believe', 'say', 'want', 'way'],\n",
       " ['everybody',\n",
       "  'rock',\n",
       "  'body',\n",
       "  'everybody',\n",
       "  'rock',\n",
       "  'body',\n",
       "  'right',\n",
       "  'backstreets',\n",
       "  'alright'],\n",
       " ['meaning', 'lonely', 'feeling', 'need', 'walk'],\n",
       " ['weve', 'fallen', 'apart', 'way', 'used', 'yeah']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_corpus = [[term for term in tokens if term not in english_stop_words] for tokens in tokenized_corpus]\n",
    "clean_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yesterday',\n",
       " 'troubles',\n",
       " 'far',\n",
       " 'away',\n",
       " 'live',\n",
       " 'yellow',\n",
       " 'submarine',\n",
       " 'yellow',\n",
       " 'submarine',\n",
       " 'times',\n",
       " 'trouble',\n",
       " 'mother',\n",
       " 'mary',\n",
       " 'comes',\n",
       " 'penny',\n",
       " 'lane',\n",
       " 'ears',\n",
       " 'eyes',\n",
       " 'comes',\n",
       " 'sun',\n",
       " 'say',\n",
       " 'alright',\n",
       " 'little',\n",
       " 'darling',\n",
       " 'ecstasy',\n",
       " 'youre',\n",
       " 'need',\n",
       " 'hey',\n",
       " 'yeah',\n",
       " 'ohh',\n",
       " 'desire',\n",
       " 'believe',\n",
       " 'say',\n",
       " 'want',\n",
       " 'way',\n",
       " 'everybody',\n",
       " 'rock',\n",
       " 'body',\n",
       " 'everybody',\n",
       " 'rock',\n",
       " 'body',\n",
       " 'right',\n",
       " 'backstreets',\n",
       " 'alright',\n",
       " 'meaning',\n",
       " 'lonely',\n",
       " 'feeling',\n",
       " 'need',\n",
       " 'walk',\n",
       " 'weve',\n",
       " 'fallen',\n",
       " 'apart',\n",
       " 'way',\n",
       " 'used',\n",
       " 'yeah']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten nested term list\n",
    "term_list = [term for sub_list in clean_corpus for term in sub_list]\n",
    "term_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(term_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sorted unique terms\n",
    "unique_terms = sorted(list(set(term_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alright',\n",
       " 'apart',\n",
       " 'away',\n",
       " 'backstreets',\n",
       " 'believe',\n",
       " 'body',\n",
       " 'comes',\n",
       " 'darling',\n",
       " 'desire',\n",
       " 'ears',\n",
       " 'ecstasy',\n",
       " 'everybody',\n",
       " 'eyes',\n",
       " 'fallen',\n",
       " 'far',\n",
       " 'feeling',\n",
       " 'hey',\n",
       " 'lane',\n",
       " 'little',\n",
       " 'live',\n",
       " 'lonely',\n",
       " 'mary',\n",
       " 'meaning',\n",
       " 'mother',\n",
       " 'need',\n",
       " 'ohh',\n",
       " 'penny',\n",
       " 'right',\n",
       " 'rock',\n",
       " 'say',\n",
       " 'submarine',\n",
       " 'sun',\n",
       " 'times',\n",
       " 'trouble',\n",
       " 'troubles',\n",
       " 'used',\n",
       " 'walk',\n",
       " 'want',\n",
       " 'way',\n",
       " 'weve',\n",
       " 'yeah',\n",
       " 'yellow',\n",
       " 'yesterday',\n",
       " 'youre']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix = pd.DataFrame(index=LABELS)\n",
    "for term in unique_terms:\n",
    "    count_matrix[term] = [sub_list.count(term) for sub_list in clean_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "#CORPUS = BEATLES_CORPUS + BACKSTREET_BOYS_CORPUS\n",
    "\n",
    "# label\n",
    "#l1,l2 = len(BEATLES_CORPUS), len(BACKSTREET_BOYS_CORPUS)\n",
    "#LABELS = [f\"beatles_{i}\" for i in range(l1)] + [f\"bboys_{i}\" for i in range(l2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alright</th>\n",
       "      <th>apart</th>\n",
       "      <th>away</th>\n",
       "      <th>backstreets</th>\n",
       "      <th>believe</th>\n",
       "      <th>body</th>\n",
       "      <th>comes</th>\n",
       "      <th>darling</th>\n",
       "      <th>desire</th>\n",
       "      <th>ears</th>\n",
       "      <th>...</th>\n",
       "      <th>troubles</th>\n",
       "      <th>used</th>\n",
       "      <th>walk</th>\n",
       "      <th>want</th>\n",
       "      <th>way</th>\n",
       "      <th>weve</th>\n",
       "      <th>yeah</th>\n",
       "      <th>yellow</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>beatles_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beatles_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beatles_2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beatles_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beatles_4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bboys_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bboys_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bboys_2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bboys_3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bboys_4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           alright  apart  away  backstreets  believe  body  comes  darling  \\\n",
       "beatles_0        0      0     1            0        0     0      0        0   \n",
       "beatles_1        0      0     0            0        0     0      0        0   \n",
       "beatles_2        0      0     0            0        0     0      1        0   \n",
       "beatles_3        0      0     0            0        0     0      0        0   \n",
       "beatles_4        1      0     0            0        0     0      1        1   \n",
       "bboys_0          0      0     0            0        0     0      0        0   \n",
       "bboys_1          0      0     0            0        1     0      0        0   \n",
       "bboys_2          1      0     0            1        0     2      0        0   \n",
       "bboys_3          0      0     0            0        0     0      0        0   \n",
       "bboys_4          0      1     0            0        0     0      0        0   \n",
       "\n",
       "           desire  ears  ...  troubles  used  walk  want  way  weve  yeah  \\\n",
       "beatles_0       0     0  ...         1     0     0     0    0     0     0   \n",
       "beatles_1       0     0  ...         0     0     0     0    0     0     0   \n",
       "beatles_2       0     0  ...         0     0     0     0    0     0     0   \n",
       "beatles_3       0     1  ...         0     0     0     0    0     0     0   \n",
       "beatles_4       0     0  ...         0     0     0     0    0     0     0   \n",
       "bboys_0         0     0  ...         0     0     0     0    0     0     1   \n",
       "bboys_1         1     0  ...         0     0     0     1    1     0     0   \n",
       "bboys_2         0     0  ...         0     0     0     0    0     0     0   \n",
       "bboys_3         0     0  ...         0     0     1     0    0     0     0   \n",
       "bboys_4         0     0  ...         0     1     0     0    1     1     1   \n",
       "\n",
       "           yellow  yesterday  youre  \n",
       "beatles_0       0          1      0  \n",
       "beatles_1       2          0      0  \n",
       "beatles_2       0          0      0  \n",
       "beatles_3       0          0      0  \n",
       "beatles_4       0          0      0  \n",
       "bboys_0         0          0      1  \n",
       "bboys_1         0          0      0  \n",
       "bboys_2         0          0      0  \n",
       "bboys_3         0          0      0  \n",
       "bboys_4         0          0      0  \n",
       "\n",
       "[10 rows x 44 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problem with simple occurance count of terms\n",
    ">- large documents will have unnecessary weight\n",
    ">- highly frequent terms without much \"information content\" may dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Term Frequency (tf)`** \n",
    "- $\\text{tf}(t,d)$ $~=~$ fraction of occurance of term $t$ in document $d$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Document Frequency (df)`** \n",
    "- $\\text{df}(t)$ $~=~$ number of documents that contain term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Inverse Document Frequency (idf)`** \n",
    "- $\\text{idf}(t)$ $~=~$ inverse document frequency of term $t$ $~=~$ $1+\\log\\left[\\frac{1+n}{1+\\text{df}(t)}\\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Term Frequency - Inverse Document Frequency (tf-idf)`** \n",
    "- $\\text{tf-idf}(t,d)$ $~=~$ $\\text{tf}(t,d) \\times \\text{idf}(t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Other Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `n_grams` hyperparameter\n",
    "- feature selection techniques, e.g., PCA\n",
    "- stemming\n",
    "- lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. BoW in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    #\n",
    "    ###################################################################\n",
    "    # convert all characters to lowercase before tokenizing\n",
    "    #\n",
    "    lowercase = True, \n",
    "    #\n",
    "    ###################################################################\n",
    "    # choose words to create features\n",
    "    #\n",
    "    analyzer = 'word',\n",
    "    #\n",
    "    ###################################################################\n",
    "    # use built-in stop word list for English (default=None)\n",
    "    #\n",
    "    stop_words = english_stop_words,\n",
    "    #\n",
    "    ###################################################################\n",
    "    # select tokens of 2 or more word characters (punctuation ignored) \n",
    "    #\n",
    "    token_pattern = r\"(?u)\\b\\w\\w+\\b\",\n",
    "    #\n",
    "    ###################################################################\n",
    "    # consider only unigrams of tokens\n",
    "    #\n",
    "    ngram_range = (1, 1)\n",
    "    #\n",
    "    ###################################################################  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = count_vectorizer.fit_transform(CORPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature matrix (document-term matrix)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix_skl = pd.DataFrame(\n",
    "    vec.todense(), \n",
    "    columns=count_vectorizer.get_feature_names(), \n",
    "    index=LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_matrix_skl "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 [`TfidfTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfTransformer(use_idf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2 = tf.fit_transform(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(vec2.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vec2.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix = pd.DataFrame(\n",
    "    vec2.todense(), \n",
    "    columns=count_vectorizer.get_feature_names(), \n",
    "    index=LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Option A: Use a CountVectorizer + TfidfTransformer sequentially in a pipeline\n",
    "- Option B: Use a TfidfVectorizer in a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=english_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec3 = vectorizer.fit_transform(CORPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix1 = pd.DataFrame(\n",
    "    vec3.todense(), \n",
    "    columns=count_vectorizer.get_feature_names(), \n",
    "    index=LABELS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learn more about [`TfidfVectorizer()`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer), and all its hyperparameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid black\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [A Gentle Introduction to the Bag-of-Words Model](https://machinelearningmastery.com/gentle-introduction-bag-words-model/)\n",
    "- [How Bag of Words (BoW) Works in NLP](https://dataaspirant.com/bag-of-words-bow/)\n",
    "- [TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python](https://towardsdatascience.com/tf-term-frequency-idf-inverse-document-frequency-from-scratch-in-python-6c2b61b78558)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
